# SGR Deep Research Agent - Configuration Template
# Copy this file to config.yaml and fill in your data


# LLM Configuration
llm:
  api_key: "your-openai-api-key-here"  # Your OpenAI API key
  base_url: "https://api.openai.com/v1"  # API base URL
  model: "gpt-4o-mini"  # Model name
  max_tokens: 8000  # Max output tokens
  temperature: 0.4  # Temperature (0.0-1.0)
  # proxy: "socks5://127.0.0.1:1081"  # Optional proxy

# Search Configuration (Tavily)
search:
  api_key: "your-tavily-api-key-here"  # Tavily API key (get at tavily.com)
  api_base_url: "https://api.tavily.com"  # Tavily API URL
  max_results: 10  # Max search results
  max_pages: 5  # Max pages to scrape
  content_limit: 1500  # Content char limit per source

# Execution Settings
execution:
  max_steps: 6  # Max execution steps
  max_clarifications: 3  # Max clarification requests
  max_iterations: 10  # Max iterations per step
  max_searches: 4  # Max search operations
  mcp_context_limit: 15000  # Max context received from MCP server

  logs_dir: "logs"  # Logs directory for agents execution
  reports_dir: "reports"  # Agent reports files directory

# Prompts
prompts:
  system_prompt_file: "prompts/system_prompt.txt"  # System prompt file path
  initial_user_request_file: "prompts/initial_user_request.txt"  # Initial request template
  clarification_response_file: "prompts/clarification_response.txt"  # Clarification template

# MCP (Model Context Protocol)
mcp:
  transport_config:
    mcpServers:
      deepwiki:
        url: "https://mcp.deepwiki.com/mcp"
        
      # Add more MCP servers here:
      # your_server:
      #   url: "https://your-mcp-server.com/mcp"
      #   headers:
      #     Authorization: "Bearer your-token"
