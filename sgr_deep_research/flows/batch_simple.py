"""Simplified Prefect flows for batch research operations."""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

from prefect import flow, task
from rich.console import Console

from sgr_deep_research.core.agents.batch_generator_agent import BatchGeneratorAgent
from .research_flow import research_flow

console = Console()
logger = logging.getLogger(__name__)


@task(name="generate-batch-plan")
async def generate_batch_plan_task(
    topic: str,
    count: int,
    languages: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Task –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞ batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""

    try:
        logger.info(f"üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –¥–ª—è {count} –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —Ç–µ–º–µ: {topic}")

        generator = BatchGeneratorAgent(
            topic=topic,
            count=count,
            languages=languages or ["ru", "en"],
        )

        batch_plan = await generator.execute()

        return {
            "status": "SUCCESS",
            "topic": batch_plan.topic,
            "total_queries": batch_plan.total_queries,
            "languages": batch_plan.languages,
            "queries": [
                {
                    "id": query.id,
                    "query": query.query,
                    "query_en": query.query_en,
                    "aspect": query.aspect,
                    "scope": query.scope,
                    "suggested_depth": query.suggested_depth,
                }
                for query in batch_plan.queries
            ],
        }

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {e}")
        import traceback

        logger.error(f"üìú Traceback: {traceback.format_exc()}")
        return {
            "status": "ERROR",
            "error": str(e),
            "traceback": traceback.format_exc(),
        }


@task(name="save-batch-plan")
def save_batch_plan_task(
    batch_plan: Dict[str, Any],
    batch_name: str,
) -> Path:
    """Task –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–ª–∞–Ω–∞ batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""

    # –î–æ–±–∞–≤–ª—è–µ–º timestamp –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    full_batch_name = f"{batch_name}_{timestamp}"

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è batch
    batch_dir = Path("batches") / full_batch_name
    batch_dir.mkdir(parents=True, exist_ok=True)

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–ª–∞–Ω - –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ = –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
    plan_file = batch_dir / "plan.txt"
    with open(plan_file, "w", encoding="utf-8") as f:
        f.write(f"# Batch: {full_batch_name}\n")
        f.write(f"# Topic: {batch_plan['topic']}\n")
        f.write(f"# Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# Total queries: {batch_plan['total_queries']}\n\n")

        for query in batch_plan["queries"]:
            f.write(f"{query['query']}\n")

    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ JSON
    meta_file = batch_dir / "metadata.json"
    with open(meta_file, "w", encoding="utf-8") as f:
        meta = {
            "batch_name": full_batch_name,
            "original_name": batch_name,
            "topic": batch_plan["topic"],
            "created": datetime.now().isoformat(),
            "total_queries": batch_plan["total_queries"],
            "languages": batch_plan["languages"],
            "queries_meta": [
                {
                    "line": i + 1,
                    "query": query["query"],
                    "query_en": query["query_en"],
                    "aspect": query["aspect"],
                    "scope": query["scope"],
                    "suggested_depth": query["suggested_depth"],
                }
                for i, query in enumerate(batch_plan["queries"])
            ],
        }
        json.dump(meta, f, ensure_ascii=False, indent=2)

    logger.info(f"üìÅ –ü–ª–∞–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {batch_dir}")
    logger.info(f"üìã –§–∞–π–ª –ø–ª–∞–Ω–∞: {plan_file}")
    logger.info(f"üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {meta_file}")

    return batch_dir


@flow(name="batch-create-flow")
async def batch_create_flow(
    topic: str,
    batch_name: str,
    count: int,
    languages: Optional[List[str]] = None,
) -> Path:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π Prefect flow –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–ª–∞–Ω–∞ batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""

    logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–∏–µ batch –ø–ª–∞–Ω–∞ '{batch_name}' –¥–ª—è —Ç–µ–º—ã: {topic}")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–ª–∞–Ω
    try:
        batch_plan = await generate_batch_plan_task(
            topic=topic,
            count=count,
            languages=languages,
        )

        if batch_plan.get("status") != "SUCCESS":
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {batch_plan.get('error')}")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {e}")
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {str(e)}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–ª–∞–Ω
    batch_dir = save_batch_plan_task(
        batch_plan=batch_plan,
        batch_name=batch_name,
    )

    logger.info(f"‚úÖ Batch –ø–ª–∞–Ω —Å–æ–∑–¥–∞–Ω: {batch_dir}")
    logger.info(f"üìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {batch_plan['total_queries']}")

    return batch_dir


@flow(name="batch-run-flow")
async def batch_run_flow(
    batch_name: str,
    agent_type: str = "sgr-tools",
    force_restart: bool = False,
    max_concurrent: int = 5,
) -> Dict[str, Any]:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π Prefect flow –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ subflows."""

    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ batch –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è '{batch_name}'")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–ª–∞–Ω
    batch_dir = Path("batches") / batch_name
    if not batch_dir.exists():
        raise FileNotFoundError(f"Batch '{batch_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤: {batch_dir}")

    plan_file = batch_dir / "plan.txt"
    meta_file = batch_dir / "metadata.json"

    if not plan_file.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –ø–ª–∞–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {plan_file}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –ø–ª–∞–Ω–∞
    queries = []
    with open(plan_file, "r", encoding="utf-8") as f:
        actual_line_num = 0
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if line and not line.startswith("#"):
                actual_line_num += 1
                queries.append((actual_line_num, line))

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–ª—É–±–∏–Ω—ã
    queries_meta = {}
    if meta_file.exists():
        try:
            with open(meta_file, "r", encoding="utf-8") as f:
                meta = json.load(f)
                for qmeta in meta.get("queries_meta", []):
                    queries_meta[qmeta["line"]] = qmeta
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {e}")

    if not queries:
        raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ (–µ—Å–ª–∏ –Ω–µ force_restart)
    queries_to_run = queries
    if not force_restart:
        completed_queries = set()
        for line_num, _ in queries:
            result_dir = batch_dir / f"{line_num:02d}_result"
            exec_file = result_dir / "execution.json"
            if exec_file.exists():
                try:
                    with open(exec_file, "r", encoding="utf-8") as f:
                        exec_data = json.load(f)
                        if exec_data.get("status") == "COMPLETED":
                            completed_queries.add(line_num)
                except:
                    pass

        queries_to_run = [(ln, q) for ln, q in queries if ln not in completed_queries]

        if completed_queries:
            logger.info(f"üîÑ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {len(completed_queries)} —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")

    if not queries_to_run:
        logger.info("‚úÖ –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")
        return {
            "status": "COMPLETED",
            "total_queries": len(queries),
            "executed_queries": 0,
            "skipped_queries": len(queries),
        }

    logger.info(f"üìã –ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é: {len(queries_to_run)} –∏–∑ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
    logger.info(f"ü§ñ –ê–≥–µ–Ω—Ç: {agent_type}")
    logger.info(f"‚ö° –ú–∞–∫—Å–∏–º—É–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö subflows: {max_concurrent}")

    # –ó–∞–ø—É—Å–∫–∞–µ–º research subflows –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    subflow_results = []

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π asyncio.gather –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è subflows
    import asyncio

    async def run_single_research(line_num: int, query: str, suggested_depth: int) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω research subflow."""
        try:
            logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ subflow {line_num}: {query[:50]}...")

            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result_dir = batch_dir / f"{line_num:02d}_result"
            result_dir.mkdir(exist_ok=True)

            # –ó–∞–ø—É—Å–∫–∞–µ–º research –∫–∞–∫ subflow
            result = await research_flow(
                agent_type=agent_type,
                query=query,
                deep_level=suggested_depth,
                output_file=str(result_dir / "report.md"),
                result_dir=str(result_dir),
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            if result.get("status") == "COMPLETED":
                meta_file = result_dir / "execution.json"
                with open(meta_file, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "line_number": line_num,
                            "query": query,
                            "agent_type": agent_type,
                            "suggested_depth": suggested_depth,
                            "completed_at": datetime.now().isoformat(),
                            "status": "COMPLETED",
                            "output_file": str(result_dir / "report.md"),
                            "stats": result.get("stats", {}),
                        },
                        f,
                        ensure_ascii=False,
                        indent=2,
                    )

                logger.info(f"‚úÖ Subflow {line_num} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return {"line_num": line_num, "success": True, "result": result}
            else:
                logger.error(f"‚ùå Subflow {line_num} –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π: {result.get('error', 'Unknown error')}")
                return {"line_num": line_num, "success": False, "result": result}

        except Exception as e:
            logger.error(f"üí• –û—à–∏–±–∫–∞ –≤ subflow {line_num}: {e}")
            return {"line_num": line_num, "success": False, "error": str(e)}

    # –°–æ–∑–¥–∞–µ–º coroutines –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    coroutines = []
    for line_num, query in queries_to_run:
        # –ü–æ–ª—É—á–∞–µ–º suggested_depth –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        suggested_depth = queries_meta.get(line_num, {}).get("suggested_depth", 0)

        coro = run_single_research(line_num, query, suggested_depth)
        coroutines.append(coro)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ subflows —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö subflows
    semaphore = asyncio.Semaphore()

    async def run_with_semaphore(coro):
        async with semaphore:
            return await coro

    logger.info(f"‚ö° –ó–∞–ø—É—Å–∫–∞–µ–º {len(coroutines)} subflows —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º {max_concurrent} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö...")

    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ subflows
    subflow_results = await asyncio.gather(*[run_with_semaphore(coro) for coro in coroutines])

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    success_count = sum(1 for result in subflow_results if result.get("success"))
    failed_count = len(subflow_results) - success_count

    logger.info(f"üéâ Batch '{batch_name}' –∑–∞–≤–µ—Ä—à–µ–Ω!")
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {success_count}/{len(subflow_results)}")
    if failed_count > 0:
        logger.info(f"‚ùå –û—à–∏–±–æ–∫: {failed_count}")
    logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {batch_dir}")

    return {
        "status": "COMPLETED",
        "batch_name": batch_name,
        "total_queries": len(queries),
        "executed_queries": len(subflow_results),
        "successful_queries": success_count,
        "failed_queries": failed_count,
        "skipped_queries": len(queries) - len(queries_to_run),
        "batch_dir": str(batch_dir),
        "results": subflow_results,
    }
